{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOdJeRn2PlNOYghChpZ/vY+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YarickVodila/Neural_network/blob/master/CNN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-adabound"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LA7zUSooo63Z",
        "outputId": "98f60ac5-49e7-487e-e142-5463b6630e47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-adabound\n",
            "  Downloading keras-adabound-0.6.0.tar.gz (5.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-adabound) (1.21.6)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-adabound) (2.8.0)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.7/dist-packages (from keras-adabound) (2.7.1)\n",
            "Building wheels for collected packages: keras-adabound\n",
            "  Building wheel for keras-adabound (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-adabound: filename=keras_adabound-0.6.0-py3-none-any.whl size=6609 sha256=a0e98315a2dbe2bd6ee40362aa475a44fec72509812edabab3d4c08e0b6fb9b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/bf/39/3d95847ef12aa58c15a6cc7a20f4f21ea39fcd52793e1beea0\n",
            "Successfully built keras-adabound\n",
            "Installing collected packages: keras-adabound\n",
            "Successfully installed keras-adabound-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muGhK87gLoMq"
      },
      "outputs": [],
      "source": [
        "from keras_adabound import AdaBound\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt #графика\n",
        "from tensorflow.keras.datasets import mnist         # библиотека базы выборок Mnist\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout,Flatten, BatchNormalization\n",
        "import time\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "#https://machinelearningknowledge.ai/keras-normalization-layers-explained-for-beginners-batch-normalization-vs-layer-normalization/#Keras_Batch_Normalization_Layer_Example"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')/255\n",
        "\n",
        "train_y = train[\"label\"].to_numpy()\n",
        "train_x = (train.drop(labels = [\"label\"],axis = 1)/255)\n",
        "train_x= np.array(train_x)\n",
        "test= np.array(test)\n",
        "print(train_x.shape)\n",
        "\n",
        "train_y = keras.utils.to_categorical(train_y, 10)\n",
        "\n",
        "\n",
        "train_x = train_x.reshape(42000, 28, 28, 1)\n",
        "test = test.reshape(-1,28,28,1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpkGiN8EL_8L",
        "outputId": "e9cd72a9-d0a0-4038-ac80-2b6bf103654b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(42000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train_x = train_x.reshape(-1,28,28,1)\n",
        "a = np.array([1,1,3])\n",
        "b = np.array([1,7,4])\n",
        "c = a+b\n",
        "print(np.argmax(c),c)"
      ],
      "metadata": {
        "id": "BG7pTTCKlPzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de055aff-721a-4d7b-8d8f-e7192c7aeb8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 [2 8 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "                      keras.layers.Conv2D(32,(3,3),padding='same', activation='relu',input_shape=(28,28,1)), # (3,3) - фильтр\n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      keras.layers.Conv2D(32,(3,3),padding='same', activation='relu'),\n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      keras.layers.MaxPooling2D((2,2),strides=(2,2)), # фильтр (2,2) для пулинга\n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      keras.layers.Dropout(0.25),\n",
        "                      \n",
        "                      \n",
        "                      keras.layers.Conv2D(64, (3,3),padding='same', activation='relu'),\n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      keras.layers.Conv2D(64, (3,3),padding='same', activation='relu'),     \n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      keras.layers.MaxPooling2D((2,2),strides=(2,2)),\n",
        "                      keras.layers.BatchNormalization(),                  \n",
        "                      keras.layers.Dropout(0.25),\n",
        "                      \n",
        "\n",
        "\n",
        "                      keras.layers.Conv2D(128, (3,3),padding='same', activation='relu'),\n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      keras.layers.Conv2D(128, (3,3),padding='same', activation='relu'),     \n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      keras.layers.MaxPooling2D((2,2),strides=(2,2)),\n",
        "                      keras.layers.BatchNormalization(),                  \n",
        "                      keras.layers.Dropout(0.25),\n",
        "                      \n",
        "\n",
        "                      keras.layers.Conv2D(256, (3,3),padding='same', activation='relu'),\n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      keras.layers.Conv2D(256, (3,3),padding='same', activation='relu'),                          \n",
        "                      keras.layers.BatchNormalization(),                  \n",
        "                      #keras.layers.Dropout(0.25),\n",
        "\n",
        "\n",
        "                      keras.layers.Flatten(),\n",
        "                      keras.layers.Dense(512, 'relu'),\n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      keras.layers.Dropout(0.25),\n",
        "                      keras.layers.Dense(512, 'relu'),\n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      keras.layers.Dropout(0.25),\n",
        "                      keras.layers.Dense(256, 'relu'),\n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      #keras.layers.Dropout(0.2),\n",
        "                      keras.layers.Dense(128, 'relu'),\n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      #keras.layers.Dropout(0.2),\n",
        "                      keras.layers.Dense(10, 'softmax')\n",
        "])\n",
        "#AdaBound(lr=0.00001)\n",
        "#keras.layers.BatchNormalization(),\n",
        "#,strides=2\n",
        "#\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy',metrics= ['accuracy'])\n",
        "#print(model.summary())              "
      ],
      "metadata": {
        "id": "0AWXaylrUNW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=5,  \n",
        "        zoom_range = 0.1,  \n",
        "        width_shift_range=0.1, \n",
        "        height_shift_range=0.1)\n",
        "his = model.fit_generator(datagen.flow(train_x, train_y, batch_size=64,  shuffle=True),epochs=100)\n",
        "\n",
        "\n",
        "#start_time = time.time()\n",
        "#his = model.fit(train_x, train_y, batch_size=250 ,epochs=50) #25 #32\n",
        "#print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "print(his.history.keys())\n",
        "#plt.plot(his.history['loss'],color = 'blue')\n",
        "#plt.plot(his.history['val_loss'],color = 'red')\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "_rU0PfQrMiWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Алгоритм ассемблера - стегинг\n",
        "n=3\n",
        "model = [0]*n\n",
        "for i in range(n):\n",
        "  model[i] = keras.Sequential([\n",
        "                      keras.layers.Conv2D(32,(3,3),padding='same', activation='relu',input_shape=(28,28,1)), # (3,3) - фильтр\n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      keras.layers.Conv2D(32,(3,3),padding='same', activation='relu'),\n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      keras.layers.MaxPooling2D((2,2),strides=(2,2)), # фильтр (2,2) для пулинга\n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      keras.layers.Dropout(0.25),\n",
        "                      \n",
        "                      \n",
        "                      keras.layers.Conv2D(64, (3,3),padding='same', activation='relu'),\n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      keras.layers.Conv2D(64, (3,3),padding='same', activation='relu'),     \n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      keras.layers.MaxPooling2D((2,2),strides=(2,2)),\n",
        "                      keras.layers.BatchNormalization(),                  \n",
        "                      keras.layers.Dropout(0.25),\n",
        "                      \n",
        "\n",
        "\n",
        "                      keras.layers.Conv2D(128, (3,3),padding='same', activation='relu'),\n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      keras.layers.Conv2D(128, (3,3),padding='same', activation='relu'),     \n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      keras.layers.MaxPooling2D((2,2),strides=(2,2)),\n",
        "                      keras.layers.BatchNormalization(),                  \n",
        "                      keras.layers.Dropout(0.25),\n",
        "                      \n",
        "\n",
        "                      keras.layers.Conv2D(256, (3,3),padding='same', activation='relu'),\n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      keras.layers.Conv2D(256, (3,3),padding='same', activation='relu'),                          \n",
        "                      keras.layers.BatchNormalization(),                  \n",
        "                      #keras.layers.Dropout(0.25),\n",
        "\n",
        "\n",
        "                      keras.layers.Flatten(),\n",
        "                      keras.layers.Dense(512, 'relu'),\n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      keras.layers.Dropout(0.25),\n",
        "                      keras.layers.Dense(512, 'relu'),\n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      keras.layers.Dropout(0.25),\n",
        "                      keras.layers.Dense(256, 'relu'),\n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      #keras.layers.Dropout(0.2),\n",
        "                      keras.layers.Dense(128, 'relu'),\n",
        "                      keras.layers.BatchNormalization(),\n",
        "                      #keras.layers.Dropout(0.2),\n",
        "                      keras.layers.Dense(10, 'softmax')\n",
        "  ])\n",
        "  model[i].compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy',metrics= ['accuracy'])\n",
        "\n",
        "for i in range(n):\n",
        "  print('Треннирую ', i+1,'модель')\n",
        "  print('==================================================================================================')\n",
        "  print('==================================================================================================')\n",
        "  datagen = ImageDataGenerator(\n",
        "        rotation_range=5,  \n",
        "        zoom_range = 0.1,  \n",
        "        width_shift_range=0.1, \n",
        "        height_shift_range=0.1)\n",
        "  his = model[i].fit_generator(datagen.flow(train_x, train_y, batch_size=64,  shuffle=True),epochs=80)       "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVnY0UUWQtD5",
        "outputId": "3a994af2-b1af-40e4-d0d3-184acf81b44d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Треннирую  1 модель\n",
            "==================================================================================================\n",
            "==================================================================================================\n",
            "Epoch 1/80\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:66: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "657/657 [==============================] - 39s 41ms/step - loss: 0.3491 - accuracy: 0.8895\n",
            "Epoch 2/80\n",
            "657/657 [==============================] - 27s 42ms/step - loss: 0.1134 - accuracy: 0.9657\n",
            "Epoch 3/80\n",
            "657/657 [==============================] - 27s 40ms/step - loss: 0.0880 - accuracy: 0.9741\n",
            "Epoch 4/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0739 - accuracy: 0.9783\n",
            "Epoch 5/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0713 - accuracy: 0.9793\n",
            "Epoch 6/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0664 - accuracy: 0.9812\n",
            "Epoch 7/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0617 - accuracy: 0.9821\n",
            "Epoch 8/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0587 - accuracy: 0.9825\n",
            "Epoch 9/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0514 - accuracy: 0.9854\n",
            "Epoch 10/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0539 - accuracy: 0.9842\n",
            "Epoch 11/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0476 - accuracy: 0.9860\n",
            "Epoch 12/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0461 - accuracy: 0.9868\n",
            "Epoch 13/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0427 - accuracy: 0.9874\n",
            "Epoch 14/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0412 - accuracy: 0.9878\n",
            "Epoch 15/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0384 - accuracy: 0.9890\n",
            "Epoch 16/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0355 - accuracy: 0.9897\n",
            "Epoch 17/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0330 - accuracy: 0.9905\n",
            "Epoch 18/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0357 - accuracy: 0.9903\n",
            "Epoch 19/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0327 - accuracy: 0.9912\n",
            "Epoch 20/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0315 - accuracy: 0.9911\n",
            "Epoch 21/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0308 - accuracy: 0.9908\n",
            "Epoch 22/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0274 - accuracy: 0.9919\n",
            "Epoch 23/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0267 - accuracy: 0.9925\n",
            "Epoch 24/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0258 - accuracy: 0.9927\n",
            "Epoch 25/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0259 - accuracy: 0.9927\n",
            "Epoch 26/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0244 - accuracy: 0.9934\n",
            "Epoch 27/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0234 - accuracy: 0.9934\n",
            "Epoch 28/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0224 - accuracy: 0.9933\n",
            "Epoch 29/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0224 - accuracy: 0.9935\n",
            "Epoch 30/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0215 - accuracy: 0.9940\n",
            "Epoch 31/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0191 - accuracy: 0.9943\n",
            "Epoch 32/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0200 - accuracy: 0.9942\n",
            "Epoch 33/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0197 - accuracy: 0.9946\n",
            "Epoch 34/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0177 - accuracy: 0.9949\n",
            "Epoch 35/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0178 - accuracy: 0.9946\n",
            "Epoch 36/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0175 - accuracy: 0.9946\n",
            "Epoch 37/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0179 - accuracy: 0.9950\n",
            "Epoch 38/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0168 - accuracy: 0.9951\n",
            "Epoch 39/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0166 - accuracy: 0.9952\n",
            "Epoch 40/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0142 - accuracy: 0.9960\n",
            "Epoch 41/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0158 - accuracy: 0.9955\n",
            "Epoch 42/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0156 - accuracy: 0.9953\n",
            "Epoch 43/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0153 - accuracy: 0.9954\n",
            "Epoch 44/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0135 - accuracy: 0.9956\n",
            "Epoch 45/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0142 - accuracy: 0.9957\n",
            "Epoch 46/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0155 - accuracy: 0.9957\n",
            "Epoch 47/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0128 - accuracy: 0.9962\n",
            "Epoch 48/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0155 - accuracy: 0.9955\n",
            "Epoch 49/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0121 - accuracy: 0.9962\n",
            "Epoch 50/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0127 - accuracy: 0.9964\n",
            "Epoch 51/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0168 - accuracy: 0.9955\n",
            "Epoch 52/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0144 - accuracy: 0.9956\n",
            "Epoch 53/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0099 - accuracy: 0.9968\n",
            "Epoch 54/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0127 - accuracy: 0.9962\n",
            "Epoch 55/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0119 - accuracy: 0.9963\n",
            "Epoch 56/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0119 - accuracy: 0.9963\n",
            "Epoch 57/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0107 - accuracy: 0.9969\n",
            "Epoch 58/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0094 - accuracy: 0.9968\n",
            "Epoch 59/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0117 - accuracy: 0.9963\n",
            "Epoch 60/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0123 - accuracy: 0.9965\n",
            "Epoch 61/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0120 - accuracy: 0.9966\n",
            "Epoch 62/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0137 - accuracy: 0.9964\n",
            "Epoch 63/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0101 - accuracy: 0.9973\n",
            "Epoch 64/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0100 - accuracy: 0.9971\n",
            "Epoch 65/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0092 - accuracy: 0.9974\n",
            "Epoch 66/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0117 - accuracy: 0.9965\n",
            "Epoch 67/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0092 - accuracy: 0.9972\n",
            "Epoch 68/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0111 - accuracy: 0.9969\n",
            "Epoch 69/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0110 - accuracy: 0.9971\n",
            "Epoch 70/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0096 - accuracy: 0.9973\n",
            "Epoch 71/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0100 - accuracy: 0.9972\n",
            "Epoch 72/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0091 - accuracy: 0.9974\n",
            "Epoch 73/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0089 - accuracy: 0.9971\n",
            "Epoch 74/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0090 - accuracy: 0.9976\n",
            "Epoch 75/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0099 - accuracy: 0.9970\n",
            "Epoch 76/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0075 - accuracy: 0.9977\n",
            "Epoch 77/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0096 - accuracy: 0.9972\n",
            "Epoch 78/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0091 - accuracy: 0.9971\n",
            "Epoch 79/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0095 - accuracy: 0.9974\n",
            "Epoch 80/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0083 - accuracy: 0.9974\n",
            "Треннирую  2 модель\n",
            "==================================================================================================\n",
            "==================================================================================================\n",
            "Epoch 1/80\n",
            "657/657 [==============================] - 29s 40ms/step - loss: 0.3284 - accuracy: 0.8956\n",
            "Epoch 2/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.1127 - accuracy: 0.9660\n",
            "Epoch 3/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0863 - accuracy: 0.9743\n",
            "Epoch 4/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0769 - accuracy: 0.9775\n",
            "Epoch 5/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0679 - accuracy: 0.9803\n",
            "Epoch 6/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0645 - accuracy: 0.9809\n",
            "Epoch 7/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0571 - accuracy: 0.9839\n",
            "Epoch 8/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0569 - accuracy: 0.9832\n",
            "Epoch 9/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0521 - accuracy: 0.9843\n",
            "Epoch 10/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0466 - accuracy: 0.9864\n",
            "Epoch 11/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0446 - accuracy: 0.9877\n",
            "Epoch 12/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0446 - accuracy: 0.9878\n",
            "Epoch 13/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0418 - accuracy: 0.9881\n",
            "Epoch 14/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0388 - accuracy: 0.9885\n",
            "Epoch 15/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0397 - accuracy: 0.9890\n",
            "Epoch 16/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0338 - accuracy: 0.9901\n",
            "Epoch 17/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0366 - accuracy: 0.9896\n",
            "Epoch 18/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0318 - accuracy: 0.9910\n",
            "Epoch 19/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0313 - accuracy: 0.9906\n",
            "Epoch 20/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0328 - accuracy: 0.9910\n",
            "Epoch 21/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0263 - accuracy: 0.9923\n",
            "Epoch 22/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0286 - accuracy: 0.9919\n",
            "Epoch 23/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0243 - accuracy: 0.9930\n",
            "Epoch 24/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0262 - accuracy: 0.9925\n",
            "Epoch 25/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0265 - accuracy: 0.9924\n",
            "Epoch 26/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0237 - accuracy: 0.9937\n",
            "Epoch 27/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0228 - accuracy: 0.9939\n",
            "Epoch 28/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0222 - accuracy: 0.9937\n",
            "Epoch 29/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0238 - accuracy: 0.9932\n",
            "Epoch 30/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0208 - accuracy: 0.9940\n",
            "Epoch 31/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0188 - accuracy: 0.9946\n",
            "Epoch 32/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0218 - accuracy: 0.9940\n",
            "Epoch 33/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0203 - accuracy: 0.9940\n",
            "Epoch 34/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0189 - accuracy: 0.9942\n",
            "Epoch 35/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0164 - accuracy: 0.9954\n",
            "Epoch 36/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0172 - accuracy: 0.9945\n",
            "Epoch 37/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0191 - accuracy: 0.9945\n",
            "Epoch 38/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0159 - accuracy: 0.9953\n",
            "Epoch 39/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0174 - accuracy: 0.9945\n",
            "Epoch 40/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0149 - accuracy: 0.9960\n",
            "Epoch 41/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0163 - accuracy: 0.9956\n",
            "Epoch 42/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0159 - accuracy: 0.9953\n",
            "Epoch 43/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0162 - accuracy: 0.9952\n",
            "Epoch 44/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0135 - accuracy: 0.9959\n",
            "Epoch 45/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0155 - accuracy: 0.9954\n",
            "Epoch 46/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0147 - accuracy: 0.9956\n",
            "Epoch 47/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0137 - accuracy: 0.9962\n",
            "Epoch 48/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0126 - accuracy: 0.9966\n",
            "Epoch 49/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0148 - accuracy: 0.9955\n",
            "Epoch 50/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0128 - accuracy: 0.9963\n",
            "Epoch 51/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0111 - accuracy: 0.9964\n",
            "Epoch 52/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0134 - accuracy: 0.9962\n",
            "Epoch 53/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0131 - accuracy: 0.9963\n",
            "Epoch 54/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0127 - accuracy: 0.9966\n",
            "Epoch 55/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0127 - accuracy: 0.9962\n",
            "Epoch 56/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0119 - accuracy: 0.9964\n",
            "Epoch 57/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0115 - accuracy: 0.9964\n",
            "Epoch 58/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0124 - accuracy: 0.9965\n",
            "Epoch 59/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0108 - accuracy: 0.9968\n",
            "Epoch 60/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0121 - accuracy: 0.9966\n",
            "Epoch 61/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0111 - accuracy: 0.9969\n",
            "Epoch 62/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0111 - accuracy: 0.9968\n",
            "Epoch 63/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0104 - accuracy: 0.9970\n",
            "Epoch 64/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0092 - accuracy: 0.9972\n",
            "Epoch 65/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0127 - accuracy: 0.9961\n",
            "Epoch 66/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0094 - accuracy: 0.9973\n",
            "Epoch 67/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0115 - accuracy: 0.9966\n",
            "Epoch 68/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0077 - accuracy: 0.9979\n",
            "Epoch 69/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0087 - accuracy: 0.9973\n",
            "Epoch 70/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0120 - accuracy: 0.9963\n",
            "Epoch 71/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0091 - accuracy: 0.9974\n",
            "Epoch 72/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0097 - accuracy: 0.9970\n",
            "Epoch 73/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0083 - accuracy: 0.9977\n",
            "Epoch 74/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0096 - accuracy: 0.9970\n",
            "Epoch 75/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0100 - accuracy: 0.9971\n",
            "Epoch 76/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0085 - accuracy: 0.9974\n",
            "Epoch 77/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0088 - accuracy: 0.9976\n",
            "Epoch 78/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0095 - accuracy: 0.9972\n",
            "Epoch 79/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0090 - accuracy: 0.9970\n",
            "Epoch 80/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0090 - accuracy: 0.9975\n",
            "Треннирую  3 модель\n",
            "==================================================================================================\n",
            "==================================================================================================\n",
            "Epoch 1/80\n",
            "657/657 [==============================] - 29s 40ms/step - loss: 0.3444 - accuracy: 0.8902\n",
            "Epoch 2/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.1129 - accuracy: 0.9665\n",
            "Epoch 3/80\n",
            "657/657 [==============================] - 27s 41ms/step - loss: 0.0866 - accuracy: 0.9733\n",
            "Epoch 4/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0757 - accuracy: 0.9779\n",
            "Epoch 5/80\n",
            "657/657 [==============================] - 27s 40ms/step - loss: 0.0692 - accuracy: 0.9804\n",
            "Epoch 6/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0651 - accuracy: 0.9811\n",
            "Epoch 7/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0571 - accuracy: 0.9839\n",
            "Epoch 8/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0562 - accuracy: 0.9843\n",
            "Epoch 9/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0558 - accuracy: 0.9843\n",
            "Epoch 10/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0489 - accuracy: 0.9863\n",
            "Epoch 11/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0461 - accuracy: 0.9865\n",
            "Epoch 12/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0422 - accuracy: 0.9883\n",
            "Epoch 13/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0422 - accuracy: 0.9881\n",
            "Epoch 14/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0390 - accuracy: 0.9892\n",
            "Epoch 15/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0376 - accuracy: 0.9896\n",
            "Epoch 16/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0352 - accuracy: 0.9899\n",
            "Epoch 17/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0368 - accuracy: 0.9896\n",
            "Epoch 18/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0353 - accuracy: 0.9905\n",
            "Epoch 19/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0315 - accuracy: 0.9915\n",
            "Epoch 20/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0289 - accuracy: 0.9921\n",
            "Epoch 21/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0302 - accuracy: 0.9913\n",
            "Epoch 22/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0271 - accuracy: 0.9918\n",
            "Epoch 23/80\n",
            "657/657 [==============================] - 27s 40ms/step - loss: 0.0266 - accuracy: 0.9926\n",
            "Epoch 24/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0269 - accuracy: 0.9929\n",
            "Epoch 25/80\n",
            "657/657 [==============================] - 26s 39ms/step - loss: 0.0250 - accuracy: 0.9929\n",
            "Epoch 26/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0249 - accuracy: 0.9928\n",
            "Epoch 27/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0238 - accuracy: 0.9933\n",
            "Epoch 28/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0215 - accuracy: 0.9937\n",
            "Epoch 29/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0211 - accuracy: 0.9938\n",
            "Epoch 30/80\n",
            "657/657 [==============================] - 27s 40ms/step - loss: 0.0217 - accuracy: 0.9940\n",
            "Epoch 31/80\n",
            "657/657 [==============================] - 27s 40ms/step - loss: 0.0202 - accuracy: 0.9939\n",
            "Epoch 32/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0184 - accuracy: 0.9948\n",
            "Epoch 33/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0197 - accuracy: 0.9945\n",
            "Epoch 34/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0181 - accuracy: 0.9947\n",
            "Epoch 35/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0197 - accuracy: 0.9943\n",
            "Epoch 36/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0171 - accuracy: 0.9951\n",
            "Epoch 37/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0175 - accuracy: 0.9952\n",
            "Epoch 38/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0175 - accuracy: 0.9952\n",
            "Epoch 39/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0152 - accuracy: 0.9951\n",
            "Epoch 40/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0173 - accuracy: 0.9950\n",
            "Epoch 41/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0155 - accuracy: 0.9955\n",
            "Epoch 42/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0160 - accuracy: 0.9954\n",
            "Epoch 43/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0149 - accuracy: 0.9954\n",
            "Epoch 44/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0159 - accuracy: 0.9951\n",
            "Epoch 45/80\n",
            "657/657 [==============================] - 27s 40ms/step - loss: 0.0161 - accuracy: 0.9952\n",
            "Epoch 46/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0149 - accuracy: 0.9958\n",
            "Epoch 47/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0131 - accuracy: 0.9963\n",
            "Epoch 48/80\n",
            "657/657 [==============================] - 27s 40ms/step - loss: 0.0130 - accuracy: 0.9960\n",
            "Epoch 49/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0124 - accuracy: 0.9963\n",
            "Epoch 50/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0132 - accuracy: 0.9963\n",
            "Epoch 51/80\n",
            "657/657 [==============================] - 27s 40ms/step - loss: 0.0135 - accuracy: 0.9963\n",
            "Epoch 52/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0109 - accuracy: 0.9970\n",
            "Epoch 53/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0144 - accuracy: 0.9959\n",
            "Epoch 54/80\n",
            "657/657 [==============================] - 27s 40ms/step - loss: 0.0122 - accuracy: 0.9963\n",
            "Epoch 55/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0125 - accuracy: 0.9965\n",
            "Epoch 56/80\n",
            "657/657 [==============================] - 27s 40ms/step - loss: 0.0118 - accuracy: 0.9964\n",
            "Epoch 57/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0108 - accuracy: 0.9967\n",
            "Epoch 58/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0118 - accuracy: 0.9965\n",
            "Epoch 59/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0114 - accuracy: 0.9961\n",
            "Epoch 60/80\n",
            "657/657 [==============================] - 27s 40ms/step - loss: 0.0108 - accuracy: 0.9967\n",
            "Epoch 61/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0108 - accuracy: 0.9969\n",
            "Epoch 62/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0114 - accuracy: 0.9968\n",
            "Epoch 63/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0120 - accuracy: 0.9967\n",
            "Epoch 64/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0099 - accuracy: 0.9970\n",
            "Epoch 65/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0103 - accuracy: 0.9968\n",
            "Epoch 66/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0106 - accuracy: 0.9971\n",
            "Epoch 67/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0103 - accuracy: 0.9967\n",
            "Epoch 68/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0095 - accuracy: 0.9969\n",
            "Epoch 69/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0094 - accuracy: 0.9971\n",
            "Epoch 70/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0114 - accuracy: 0.9965\n",
            "Epoch 71/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0100 - accuracy: 0.9972\n",
            "Epoch 72/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0095 - accuracy: 0.9971\n",
            "Epoch 73/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0096 - accuracy: 0.9972\n",
            "Epoch 74/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0091 - accuracy: 0.9974\n",
            "Epoch 75/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0100 - accuracy: 0.9970\n",
            "Epoch 76/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0089 - accuracy: 0.9975\n",
            "Epoch 77/80\n",
            "657/657 [==============================] - 27s 40ms/step - loss: 0.0091 - accuracy: 0.9972\n",
            "Epoch 78/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0093 - accuracy: 0.9972\n",
            "Epoch 79/80\n",
            "657/657 [==============================] - 26s 40ms/step - loss: 0.0088 - accuracy: 0.9973\n",
            "Epoch 80/80\n",
            "657/657 [==============================] - 27s 40ms/step - loss: 0.0090 - accuracy: 0.9975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = np.zeros( (test.shape[0],10) ) \n",
        "for j in range(n):\n",
        "  results = results + model[j].predict(test)\n",
        "sample = pd.read_csv('sample_submission.csv')\n",
        "prediction = np.array(pd.DataFrame(results).idxmax(axis=1))\n",
        "sample['Label']=prediction\n",
        "sample.to_csv('sample_submission.csv',index=False)\n",
        "print(sample)\n",
        "#results = np.argmax(results,axis = 1)\n",
        "#results = results.astype(int)\n",
        "#print(results)\n"
      ],
      "metadata": {
        "id": "K2O0HjnKR61k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "prediction = model.predict(test)\n",
        "prediction = np.array(pd.DataFrame(prediction).idxmax(axis=1))\n",
        "\n",
        "sample['Label']=prediction\n",
        "sample.to_csv('sample_submission.csv',index=False)\n",
        "\n",
        "print(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXRpdrJvUuXi",
        "outputId": "f70d86be-248e-4eb3-c681-8b4faf8301f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       ImageId  Label\n",
            "0            1      2\n",
            "1            2      0\n",
            "2            3      9\n",
            "3            4      0\n",
            "4            5      3\n",
            "...        ...    ...\n",
            "27995    27996      9\n",
            "27996    27997      7\n",
            "27997    27998      3\n",
            "27998    27999      9\n",
            "27999    28000      2\n",
            "\n",
            "[28000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0.9996190667152405"
      ],
      "metadata": {
        "id": "DGajbo6uXX3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train, train), (test_x, test_y) = mnist.load_data()\n",
        "#train_x = train_x /255   train_x, train_y\n",
        "#train_y = keras.utils.to_categorical(train_y, 10)\n",
        "test_x = test_x /255 \n",
        "test_y = keras.utils.to_categorical(test_y, 10)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(train_x, train_y)\n",
        "print('Тест точность = ', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ak4IGdvsM_RZ",
        "outputId": "2bc6a4bc-ee9a-448a-adcd-b67327f9cd7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1313/1313 [==============================] - 6s 4ms/step - loss: 0.0096 - accuracy: 0.9969\n",
            "Тест точность =  0.9969047904014587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "tit = dataTest.iloc[: ,:1]\n",
        "tit['Survived'] = prediction[0]\n",
        "tit.set_index('ImageId', inplace=True)\n",
        "tit = tit.astype({'Survived': np.int})\n",
        "print(tit)\n",
        "tit.to_csv('Submit.csv', sep=',')'''"
      ],
      "metadata": {
        "id": "rN6grFPoU2XR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VVacp3EGlcPs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}